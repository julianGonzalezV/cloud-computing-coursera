


::::::::::::::::::;mapReduce Paradimg:::::::::::::::::


ejemplo de mapReduce: Supongamos que se desea obtener la suma de los cuadrados de una lista de enteros

1) aplicamos el map
(map square (1 2 3 4)) 
; ( 1 4 9 16 ) procesa cada registro o cada elemento secuencialmente PERO DE MANERA INDEPENDIENTE, es decir que el square de 2 se puede calcular independientemente del square de 3 , npara squre de 3 no se requiere de la salida de square de 2.

2) Luego aplicariamos el reduce
(reduce + (1 4 9 16))
; 30 , procesa los registros en batches o grupos de elementos , puede que en un mismo grupo queden todos los elementos o que se generen varios subgrupos de todos los elementos en total 


SER[A QUE ADELANTANDONOS PODRIAMOS PENSAR QUE EL MAP HACE QUE SE EJECUTEN LOS COMPUTOS INTEPENDIENTES EN CADA NODO Y EL REDUCE VA POR TODOS ESOS RESULTADOS Y GENERA LA RSPUESTA???
Al completar el curso o semana ver si es o no esto :) 





Otro Ejemplo:
Que tal s[i nos pidieran que dado un texto y una palabra , busquemos las veces en que dicha palabra se encuentra en el texto  , ejemplo del como lo hariamos aplicando mapReduce:

Pensar en el caso en que nos pasen un texto demasiado enorme....

1) Aplicando Map: (que por casualidad genera un map pero no siempre es as[i)

	Procesando registros individuales para generar una estructura  Key value, en este caso cada registro ser[ia un word 


ejemplo dado: un archivo con 2 l[ineas en donde sus valores ser[an los siguientes
Welcome everyone 
hello everyone 

el resultado ser[ia {Welcome 1 , everyone 1, hello 1, everyone 1 }  (el segundo everyone 1 lo que se hace es que a cada word se le da un peso de 1 que es cuando fu[e encontrado en el texto)

iMAGINEMOS que las l[ineas 1 y 2 del archivo contiene textos enormes o que el archivo tiene demasiadas l[ineas ;
	> Se puede paralelizar por l[ineas o conjunto de l[ineas para que sea m[as eficiente el proceso y al final que entreguen el Map Key - value que se requiere, ya que son procesos indepentientes y el que se genere el map de la l[inea 1 no es necesario para que se genere la 1 se puede hacer independientemente.

2) Aplicando reduce:::::::
>Hasta ahora hemos visto que solo se genera un Map y los keys repetidos tienen el valor de 1, por lo cual entra a jugar el reduce , en donde la salida es otro Map , en donde acumula los values de los keys repetidos durante la FASE MAP paso 1.

el resultado ser[ia algo asi como:
{Welcome 1 , everyone 2, hello 1 }, en este los values ya ser[ian los conteos 

Y como paralelizar el reduce fase para [este ejemplo??

Asignando keys a los reducer por lo cual un key puede ser everyone entonces ese reduce solo ir[a por ese key en el map del paso 1 

En el video me enrredo porque quedan 2 reducer  en paraleo pero no deber[ian ser 3? por lo que al crear los reduces me dan 3 diferentes


OTRA FORMA es aplicar hash partitioning:::::::::::
que consiste en:

1 Se toma el key

2 Se le aplica hashing con una funci[on hash. un algoritmo simple ono to one o Message o MD Five, o cualquiera , siempre y cuando sea consistente, es decir que la misma se aplica a todos los Keys

3 Se toma el numero de los Reducers (reduce servers ) en este caso el nuemero de cada tarea tipo Reduce.


Cual es la principal diferencia entre MAPs y REDUCERs?

Map procesa Keys de manera independiente mientras Reducers van pro conjuntos que se relacionan con el mismo key.

MapReduce comes from Google, ellos no lo dejaron como opensource pero su dejaron un paper con la explicaci[on acerca de [este ; luego ingenierso de yahoo lo tomaron e implementaron su propio que  se convirti[o en Apache hadoop y es Opensource.


Ver en el video 3.1 minuto 11:37 reduce paradigm la implementaci[on de MAP para el ejemplo anterior , que devolveria un mam con Key =  la palabara que se est[a procesando ; value =1 

Igual en el minuto 11:37 est[a la implementaci[on del reduce julian anfres gonzalez

Tambien est[a la implementacion del driver co c[odigo que ejecutar[a las dos fases Map and Reduce
	recibe un archivo de Outut y otro de Input, es algo que maneja Hadoop.


:::::::::::::::::::::::::::::::::::::::::::::::::::::::::VIDEO 3.2 MAP REDUCE EXAPMPLE:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Ejemplos de algunas aplicaciones que lo usan y lo potente que puede llegar a ser:

1) Distributed Grep  (comamdo linux)
Input : gran cantidad de archivos 
Output: Lineas que coinciden con el patron

Map: para emitir las lineas que complen 
Reduce:  para copiar la salida
> En esta aplicacion por ejemplo se puede tener Map Task paralelizados y hasta el reduce porque no depende de alguna línea en específico.


2) Reverse Web link Graph
 Input : Tuplas (a, b) donde  (page a -> page b ), la pagina a (origen)se relaciona o tiene un link a la pg b (destino)
 Output : Por cada pagina se debe olbtener las listas de paginas con las que se relaciona (tiene un link)

 Map : por cada par <source, target>, entonces devuelve el listado <target , source> 
		Por que invierte los keys? R/ Diría que es debido a que es la entrada ideal para aplicar el reduce
		
 Reduce: Recie la salida del proceso Map (invertir Keys) y los relaciona por grupos
 
 Se podría crear una extensión de este ejemplo para que diga cuantas páginas apuntan el mismo destino, con esto solo sería que la fase Map retorne por cada key
 <target , 1> y en la fase Reduce se sumarían.
 
 Quedo en Min 4:15
 
3) Conteo de urls accedidas de manera frecuente 
 
 Input  : Logs de Urls accedidas, puede ser desde el proxy server
 Output:  Porcentaje de accesos por cada URL del log

Map: para agarrar cada url y darlo un peso eje <url, 1>

Reduce: se agrupa por cada url y la suma de los conteos, hasta ac]a nada  diferente. peeero ... se necesita es el porcentaje y tend]ia que ser dividido por la cantidad total de Url distintas 
....por lo anterior se requiere OTRA FASE DE MAP REDUCE.

Map2 : para retornar <1, (url, count)>...<1, (url_n, count_n)>, para que en la fase de reduce se sumen esos 1nos

Reduce 2 : <url, count/over_all_count>, el reducer2  har[ia el pason para calcula el porcentaje.


este es un ejemplo de como encadenar Map reducers





NOTA: 
Map fase output es una resultado ordenado (eg quicsort)

Reduce fase output es una resultado ordenado (eg mergeSort)

4) SORT algorithm

Input: serie de (key, value) pairs

Output: sorted VALUEs.



Map : <key, value> => <value,_> (_ es la identidad)

Reduce: <key, value> => <key, value> , esto significa que el reduce no hace nada sino que retorna el mismo map  <key, value> PORQUE LA FASE DEL MAP YA LO HIZO, ADEM]AS EN ESTA FASE
POR DEFECTO VUELTE Y ORDENA MERGE

 CUALES SON LAS VENTAJAS DE QUE LAS FASES MAP AN REDUCE ORDENEN SUS SALIDAS?

> hACER SORT resulta mucho mas f[acil 
> Un Map puede saber que continuo set de datos puden ser enviados a que espefico Reduce 
> Reduce puede llamar la funcion de reducci[on sobre un contiguo ser de datos 



::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::



:::::::::::::::::::::::::::::::::::::::::VIDEO 3.3 MapReduce Scheduling:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Programando Map Reduce:
Externamente:
programar el Map, el reduce : enviarle el trabajo a realizar y esperar una respuesta (lo anterior es muy abstracto porque no se requiere saber como paralelizar Maps and Reducers)


Internamate: Si se debe preocupar de como paralelizar, que primero se ejecuten los Maps y luego los reducers.




FOR THE CLOUD:

1 Paralelizando un Map 
	> Recordar que cada task on Map es independiente.
		Asegurarnos que los outputs de los Maps que contengan el mismo Key sea pasado a un mismo Reduce
2 Transfer data from Map to the Reduce 
		Usar una funcion de particionamiento, por ejemplo hash(key) % cantidad de reducers


3 Paralelizando un Reduce:
	> Tambien son tareas independientes, ya que a cada reducer se le asigna un set de keys y no se espera a que un set de keys sea procesado.
	>  Los reducer no se sobrelapan entre si.


Por lo anterior, schedulizar Map and reduce no deber[ia ser complicado en cuanto un Map no se habla son otro Map y un Reducer no se habla con el otro.


4 Implement Storage for input and output Map : Input and output Reducers


 
::::::::::::::THE YARN SCHEDULER (yet another resource Negociator)

eS Un nuevo scheduler que est[a siendo usado en Apache Hadoop 

Containers son algo de CPU + memoria

Cada Server es una colecci[on de contenedores 

Por lo anterior si un pc tiene 4 cores y 4GB de RAM  entonces EN UN SOLO CONTAINER habr[a 1 core y  1GB de RAM, Por lo cual ese servidor tendr[a 4 contenedores , en donde se podr[an ejecutar
a lo sumo 4 tareas al mismo tiempo en cada contenedor


YARN tIENE 3 COMPONENTES principales:

1 Global Resource Manager (RM): Hay un solo  Global Resource Manager, quien es el que inicia el scheduler

2 Per-server Node Manager (NM): 1 NM por servidor, ese el deamon encargado de ese servidor en especifico y encargado del monitoreo de fallos de tareas ejecutadas en ese servidor.

3 Per application job  Application Master (AM): 1 AM por servidor y es el encargado de negociar CONTENEDORES  con el RM y NM , SI hay tareas ejecutandosen all[i , si se est[an o no ejecutando 


EL Application Master (AM) ES EL ENCARGADO DE UN JOB A LA VEZ


:::::::::::::::::::::::::::::::::::::::::VIDEO 3.4 Fault Tolerance:::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Server Failures : Este es el m[as com[un , y afecta el resto de componentes YAR (ver arriba), el server corre dichos compoenentes por cada Job
Para el tratamieto de Falllas del lado del servidor existen los Hearbeats (en este caso seniales que se emiten a otros compoenentes Yarn )
 >El NM(uno por cada servidor) env[ia estas seniales periodicamente al RM , por lo anterior si el servidor falla la senial se deja de emitir y el RM se da cuenta que el server dej[o de funcionar
 >EL RM notifica al AM del fallo y este actua tomando una accion al respecto
 >De igual forma si el AM falla para los heartbeats hacia el RM y el RM se da cuenta del fallo y lo que hace es reiniciar el AM para que retome sus tareas. Lo anterior puede ser complicado , dependiendo de la cantidad de tareas que se encontraba ejecutando. 

Ven alguna dependencia?? ..si es RM ..

Global Resource Manager (RM) failures: No hay forma de detectar los fallos de los RM, pero una forma de tratar con esto es tener un BACKUP  del RM , el cual  ser[ia el encargado ( de manera inmediata) una vez el RM falla.



::::::;straggler (Slow servers or nodes):
> The slowest machine slow the entire job down : IF el proceso map o Reduce que se est[e corriendo en dicha m[aquina esta lento entonces, si espor el lado del Map toda la operacipon Reduce No 
puede iniciar y si es por el lado del Reduce la ejecuci[on del job en general se para hasta que no se ejecute hasta el ultimo Reducer

> Que los hace lentos?  , Un disco daniado , la red lenta , cpu, memoria 

Como se trata este problema?
Teniendo una traza del progreso de las tareas y el AM toma la decicion de hacer , Ejemplo:

Para un Job  tenemos 3 tareas ejecutandosen y una de ellas est[a en 10% y nada que avanza, por lo cual el AM genera una replica de dicha tarea en otro servidor (ejecutando el bk de esta) y la primera que 
termine es la que se toma como Ejecutada y el ultimo se Elimina (kill) y la tareas se ,arcar[a como completa. esto se conoce como Speculated Execution DEBERIA LLAMARSE REPLICATED EXECUTION ya que no se 
esta especulando nada.



::::::;Locality 

Una Nube por lo general tiene una topolog[ia Jer[arquica ejemplo RACKS
GFS/HDFS almacenan 3 replicas en el rack deseado por ejemplo 2 replicas en uno y la restante en otro. Lo anterior por si uno de los Racks falla entonces se puede reprogramar las tareas en otro que ya
contiene la replica, O EN:

 eN EL SIGUIENTE ORDEN:

1) En otro que contenga la replica (PREFERIBLEMENTE)

2) Que MapReduce intente programar una tarea en el mismo rack que fall[o 

3) en cualquier lugar

 
Hadoop Distributed File System (HDFS) is an Apache project. It's a file system which is used to store the initial and 'reduced' data once the data is processed using MapReduce. Google File System (GFS) was the database created by Google initially to store the website indexing data for the search engine. 
HDFS is based on GFS.




Entrevista a Sumeet Singh:


Mucho enfasis en que los siguientes anios ser[an muy importantes respecto a Cloud computing, Cloud computing abre paso a Nuevas y mejoradas experiencias de usuario 
disponibilidad , repidez , Incluso hablaba de 




::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

